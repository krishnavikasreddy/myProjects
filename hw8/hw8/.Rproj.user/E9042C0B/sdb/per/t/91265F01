{
    "collab_server" : "",
    "contents" : "---\ntitle: \"hw8\"\nauthor: \"Krishna Vikas\"\ndate: \"November 12, 2016\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n```{R}\nlibrary(data.table)\ngrocerry=fread(\"grocerry.txt\")\nconsultant = fread(\"consultant.txt\")\napt_test=fread(\"tests.txt\")\nsetnames(grocerry,c(\"y\",\"x1\",\"x2\",\"x3\"))\nsetnames(consultant,c(\"y\",\"x1\",\"x2\"))\nsetnames(apt_test,c(\"y\",\"x1\",\"x2\",\"x3\",\"x4\"))\napt_test_valdiate=fread(\"t2.txt\")\nsetnames(apt_test_valdiate,c(\"y\",\"x1\",\"x2\",\"x3\",\"x4\"))\n\n\n```\n\n#7.4\n## Refer to Grocery retailer Problem 6.9.\n## a. Obtain the analysis of variance table that decomposes the regression sum of squares into extra sums of squares associated with X,; with X 3 , given X,; and with X 2 , given X, and X .\n```{R}\nmodel1=lm(y~x1+x3+x2,data=grocerry)\nanova(model1)\n## form the above\n## SSE(X_1) = 136366 \n## SSE(X_3|X_1)=2033565\n## SSE(X_2|X_1, X_3)=6675\n```\n## b. Test whether X 2 can be dropped from the regression model given that X, and X 3are retained.Use the F* test statistic and ex = .05. State the alternatives, decision rule, and conclusion.What is the P-value of the test?\n\n### model => Y = B_0 + B_1 X_1 + B_3 X_3 + B_2 X_2 +e\n\n### Hypothesis\n\nH_0 => B2 = 0\nH_a > B2 != 0\n\n### test\nF* = MSE(X_2|X_1, X_3)/MSE(X1,X2,X3)\n = 6675/20532\n = 0.325102279\n\nF(0.95,1,48) = 4.042652\n\nF* < F(0.95,1,48)\nConclude H_0 \n\nP-value = 0.57123\n\n##Decision\nCan remove X_2\n\n## c. Does SSR(X,) +SSR(X2 IX,) equal SSR(X 2 ) +SSR(XtlX2 ) here? Must this always be the case? \n\n```{R}\nanova(lm(y~x1+x2,data=grocerry))\n## SSR(X_1)+SSR(X_2|X_1) = 136366+5726 = 142092\n\nanova(lm(y~x2+x1,data=grocerry))\n## SSR(X_2)+SSR(X_1|X_2) = 11395+130697 = 142092\n```\n\nThis should always be the case because error eplined by one predictor variable is complimented by the other \n\n#------------------------------------------\n\n#7.13\n## Refer to Grocery retailer Problem 6.9. Calculate R~I' R~2' Rf2' Rh 12' R~2P' R~2113' and R2. Explain what each coefficient measures and interpret your results.\n\n\n```{R}\n#Y~x1\nsummary(lm(y~x1,data=grocerry))\n# R^2_Y1 = SSR(X_1)/SSTO = 136366/3025770 = 0.04312\n## amount of error explained by x1\n\nsummary(lm(y~x2,data=grocerry))\n# R^2_Y2 = SSR(X_2)/SSTO =11395/3150741 = 0.00361661\n## amount of error explained by x2\n\nsummary(lm(x1~x2,data=grocerry))\n#R^2_12  = 0.007207\n## explains the correlation between x1 and x2\n\n## R^2_{Y1|2} = SSR(X_1|X_2)/SSE(X_2)\n## = 130697/3150741 = 0.041481353\n## amount of error explained by x1 after left over in the model with x2\n\n## R^2_{Y2|1} = SSR(X_2|X_1)/SSE(X_1) \n## =5726/3025770 = 0.001892411\n\n## amount of error explained by x2 after left over in the model with x1\n\n##R^2_{Y2|13} = SSR(X2|X1,X3)/SSR(X1,X3,X2) = 0.006773005\n\nmodel=lm(y~.,data=grocerry)\nanova(model)\n## R^2 = 0.688\n## this means 68% of total sum of squares is explained by the the regression model(y~X1+X2+X3)\n```\n\n#---------------------------------------------\n#7.17\n## a. Transfonn the variables by means of the correlation transformation (7.44) and fit the standardized regression model (7.45).\n```{R}\nx_matirx=cor(grocerry)[2:4,2:4]\ny_matrix=cor(grocerry)[1,2:4]\n\nb_matrix=solve(x_matirx,y_matrix)\nprint(b_matrix)\n```\n## Y=BX =>\n## here B=b,\n\n## b. Calculate the coefficients of determination between all pairs of predictor variables. Is it meaningful here to consider the standardized regression coefficients to reflect the effect of one predictor variable when the others are held constant?\n\n```{R}\n##R12\nsummary(lm(x1~x2,data=grocerry))\n\n##R23\nsummary(lm(x2~x3,data=grocerry))\n\n##R13\nsummary(lm(x1~x3,data=grocerry))\n\n## the co-efficients before standardisation and after standardisation only differ by scale -- hence can be said the same as before\n\n## c. Transform the estimated standardized regression coefficients by means of (7.53) back to the ones for the fitted regression model in the original variables. Verify that they are the same as the ones obtained in Problem 6. lOa.\nb_1=(sd(grocerry$y)/sd(grocerry$x1))*b_matrix[1]\nb_1\nb_2=(sd(grocerry$y)/sd(grocerry$x2))*b_matrix[2]\nb_2\nb_3=(sd(grocerry$y)/sd(grocerry$x3))*b_matrix[3]\nb_3\nb_0=mean(grocerry$y)-(b_1*mean(grocerry$x1))-(b_2*mean(grocerry$x2))-(b_3*mean(grocerry$x3))\nb_0\nlm(y~x1+x2+x3,data=grocerry)\n## they are same\n\n```\n\n# 8.24\n## Plot the sample data for the two populations as a symbolic scatter plot. Does the regression relation appear to be the same for the two populations?\n\n```{R}\nplot(consultant$x1,consultant$y)\nabline(lm(y~x1,data=consultant[consultant$x2==1]),col=\"green\")\nabline(lm(y~x1,data=consultant[consultant$x2==0]),col=\"red\")\n\n## as we can see the regression lines differ,so thr relation is different\n\n## b. Test for identity of the regression functions for dwellings on comer lots and dwellings in other locations; control the risk of Type I error at .05. State the alternatives, decision rule,and conclusion.\n```\n\n\\alpha = 0.05\n\nmodel\ny=b_0+b_1 x1+b_2 x2+b3 x_1*x_2\n\nif there is no relation then both b2 and b3 should be zero\n\nso h_0 > b_2 = b_3 =0\nh_a > b_2 !=0 and b_3 !=0\n\nF* = MSR(x2,x1x2|x1)/MSR(x1,x2,x1x2)\nMSR(x2,x1x2|x1) = SSR(x2|x1)+SSR(x1x2|x1,x2)/2\n=453.1+113/2\n=566.1/2 =283.05\n\nMSR(x1,x2,x1x2) = 909/60=15.2  \n\nF*= 283.05/15.2 =18.62 >F(0.95,2,60) = 3.15\nSo we reject null hypothesis\n\n# Decision\nWe conclude that there are different and not identical\n\n\n\n## c. Plot the estimated regression functions for the two populations and describe the nature of the differences between them.\n\n```{R}\nlm(y~x1,data=consultant[consultant$x2==0])\nlm(y~x1,data=consultant[consultant$x2==1])\n\n## there is difference in both the intercept and slope so wher x2=1 prices increase at a slower pace while x2=0 prices increase at a faster pace\n```\n\n# 9.10\n## a.Prepare separate stem-and-Ieaf plots of the test scores for each of the four newly developed aptitude tests. Are there any noteworthy features in these plots? Comment\n\n```{R}\nstem(apt_test$y,scale=10)\nstem(apt_test$x1,scale=10)\nstem(apt_test$x2,scale=10)\nstem(apt_test$x3,scale=10)\nstem(apt_test$x4,scale=10)\n\n## therse seems to be some outliers in x1,x3\n## notice that the scale is almost same for all the varibles- \n\nplot(apt_test)\n# y and x3,y and x4 seems to have a strong relation - graph is almost linear\n\n## b. Obtain the scatter plot matrix. Also obtain the correlation matrix of the X variables. What do the scatter pi ots suggest about the nature of the functional relati onsli.ip between the response variable Y and each of the predictor variables? Are any serious multicollinearity problems evident? Explain.\n\nplot(apt_test)\n# y and x3,y and x4 seems to have a strong relation - graph is almost linear\n\ncor(apt_test)\n## from the scatter plot it is evidentn that x1 and x4 has a linear relation, also there seems to be a linear relation between x1, and x2,x3, although cor values are no those high but for  ,cor(x1,x4) =0.89, indicatiing that it might lead us to problems.Suggests that co-linear transformation might be needed\n\n## c. Fit the multiple regression function containing all four predictor variables as first-order terms. Does it appear that all predictor variables should be retained?\n\napt_model=lm(y~x1+x2+x3+x4,data=apt_test)\napt_model\nanova(apt_model)\nsummary(apt_model)\n\n## From the above data, it seems that x2 need not be in the model, its pvalue indiate evidence against it\n\n## if we dig deeper x1 and x3 has very high SSR,it is explaining the major part of the error, so may be ther is a higher order in the form of x1 and x3 that could explain y in whole..we need to investigate\n```\n\n# 9.11\n## a. Using only first-order terms for the predictor variables in the pool of potential X variables,find the four best subset regression models according to the R~.p criterion.\n\n```{R}\n\nfor(n1 in 1:4){\nprint(paste(paste0(\"x\",n1),summary(lm(y~get(paste0(\"x\",n1)),data=apt_test))$adj.r.squared))\n}\n\nfor(n1 in 1:3){\n    for(n2 in (n1+1):4){\n        print(paste(paste0(\"x\",n1,\",x\",n2),(summary(lm(y~get(paste0(\"x\",n1))+get(paste0(\"x\",n2)),data=apt_test))$adj.r.squared)))\n    }}\n\nfor(n1 in 1:2){\n    for(n2 in (n1+1):3){\n      for(n3 in (n2+1):4){\n        print(paste(paste0(\"x\",n1,\",x\",n2,\",x\",n3),(summary(lm(y~get(paste0(\"x\",n1))+get(paste0(\"x\",n2))+get(paste0(\"x\",n3)),data=apt_test))$adj.r.squared)))\n    }}}\n    \nsummary(lm(y~x1+x2+x3+x4,data=apt_test))$adj.r.squared\n\n## from the above we have \n## X1, X3, X4 = .9560\n## X1, X2, X3, X4 = .9555\n## X1, X3 = .9269\n## X1, X2, X3 = .9247\n\n```\n\n## b. Since there is relatively little difference in R~.p for the four best subset models, what other criteria would you use to help in the selection of the best model? Discuss.\n\nR^2_ap penalises models having large number of predictors,AIC and SBC penalises for adding a predictor, so they can be used additional to R^2_ap for model sel2ection\n\n# *9.18.\n\n## a. Using forward stepwise regression, find the best subset of predictor variables to predict job proficiency. Use ct limits of .05 and .lD for adding or deleting a variable, respectively.\n\n```{R}\nnull=lm(y~1,data=apt_test)\nfull=lm(y~.,data=apt_test)\n\n\n## forward\n## we follow the forward step regression model principles of ~ add those variables that have high F Values and Low P Values\n## stop when the F Values fails the significance level test\n\nadd1(null,scope = c(\"x1\",\"x2\",\"x3\",\"x4\"),test=\"F\")\n# x3 has highest F Value and lowest P Value\nadd1(lm(y~x3,data=apt_test),scope = c(\"x1\",\"x2\",\"x4\"),test=\"F\")\nadd1(lm(y~x3+x1,data=apt_test),scope = c(\"x2\",\"x4\"),test=\"F\")\n# P values is lo so we add\n\nadd1(lm(y~x3+x1+x4,data=apt_test),scope = c(\"x2\"),test=\"F\")\n## we dont add x2 here becasue the F values are below the signifiance level and a very high P-Value\n## the model is y~x3+x1+x4\n\n```\n## How does the best subset according to forward stepwise regression compare with the best subset according to the R~.p criterion obtained in Problem 9.11 a?\n\nstep wise regression gives us only one model and we go with an assumption that this is the best,may not be the case, given the real world problems where a particualar variable can be vaery important.\n\nbut using adjusted-R-Squared criteria, we have many options and we can choose according to the problem\n\nin this particualr case both the ways tells us that the y~x1+x2+x3 is a good model\n\n\n\n# *9.21.\n\n## Refer 10 Job proficiency Problems 9.10 and 9.1 X. To as.~ess iJ1lcrnally the predictive ability of the regression model identified in Problem 9.1 X. compute the PI?ESS statistic and COnlpare it to SSE. What docs this compaL\"ison suggest about the validity of MSE as an indiC1uor of the predictive ability or the fitted mode!'?\n\n```{R}\nPRESS_v <- function(model) {\n  pr = residuals(model)/(1-lm.influence(model)$hat)\n  PRESS <- sum(pr^2)\n  return(PRESS)\n}\n\nmodel921=lm(y~x3+x1+x4,data=apt_test)\nPRESS_v(model921)\nanova(model921)\n```\n\n### from the above we can see that SSE < PRESS\n\nobviously SSE is calculated by the model that contains the data point, so the model is optimised for the data point, however PRESS is calculated by removing the data point and thus the data point becomes predictive quantity, and thus increasign the error probability\n\n### MSE\n\nMSE tells about the errors in the current data, for which the reduction of errors takes place.But to tell some thing about the predicive ability of the model using MSE may not be a good idea,Since it no way considers how the model reacts to a new data point\n\nas long as the predictive data is near to the fitted data,MSE works fine as predictive indicator, but for the data that is faraway from the fitted data MSE is a bad indicator.While model validating we need to consider that data which is faraway from the fitted data.so to sum up MSE is not so good as to eastimate the predictive ability of the model\n\n\n## a.Obtain I he correlation matrix of the X variables for the validation data set and compare it with that obtained in Problem 9.1 Ob for the model-building data set. Are the two cOl\"L-elation mutrices reasonably similar?\n\n```{R}\ncor(apt_test)\ncor(apt_test_valdiate)\n## not reasonable similar as cor(x1,x2), cor(x2,x3),cor(x2,x4),cor(x3,x4) differ by a decimal point which is almost 10% difference\n```\n\n## b. Fit the Legression model identified in Problem 9.1 ga to the validation dl1ta set. Compare the estimated regression coefficients and their estimated standard deviations to those obtained in Problem 9.18a. Also compme the elTor mean squares ,md coefficients of multiple de-termination. Do the estimates for the validation dat,1 set appear to be reasonably similar to those obtuined for the model-building data set?\n\n```{R}\nmodel9221 = lm(y~x3+x1+x4,data=apt_test_valdiate)\nmodel9222 = lm(y~x3+x1+x4,data=apt_test)\n\nsummary(model9221)\nsummary(model9222)\n\n# Regression Co-efficients are almost similar with very less change in slope and intercept\n\nvcov(model9221)\nvcov(model9222)\n# diagonals give the SD of the regression coefficients\n\n#the standard devition has changed by a significant percent, with the validation modal having less ,indicatin that the error rate of validation set is low,thus the model is a good fit\nanova(model9221)\nanova(model9222)\n\n# MSE is higher for the validation data almost by 11% so falls our alpha =0.10 threshold\n\n# R^2 indicate both are good model fits - much cannot be said as both are high >0.9\n\n# Significant difference between the SSE of X3 and X4,Remember the model has x3 as main predictor\n```\n\n## c. Calculate the mean squared prediction errOL' in (9.20) and compare it to MSE obtained fmm the model-building dma sel. Is there evidence of a substantial bias problenl in MSE here? Is this conclusion consistent with your finding in Problem 9.21? Discuss.\n\n```{R}\n\ny1_pred=predict.lm(lm(y~x3+x1+x4,data=apt_test),apt_test_valdiate)\nmspr=sum((apt_test_valdiate$y-y1_pred)^2) / (nrow(apt_test_valdiate))\nprint(mspr)\n\n# As in this case MSPR is low compared to SSE ,but a preliminary analysy show that much of the data is in similar range of the fitted model\ntest112=rbind(apt_test,apt_test_valdiate)\nplot(test112)\n\n## the above figure which contains combination of both the test and validation data sho that .. the validation data is very close to the model data, thus our model is good for the prediction data\n```\n\n## d. Combine the model-building data set in Problem 9.10 with the valid1Ltion data set and fit the selected LegLession model to the combined data. AL'e the estimuted stundurd deviations of the estimated L'egL'ession coefficients appreciably leduced now from those obtained for the model-building data set?lm(y~x3+x1,data=test112)\n```{R}\ntest112=rbind(apt_test,apt_test_valdiate)\nvcov(lm(y~x3+x1+x4,data=test112))\nvcov(model9222)\n\n# from above we can see that they reduced significantly\n```\n",
    "created" : 1479137874376.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2361437980",
    "id" : "91265F01",
    "lastKnownWriteTime" : 1479158032,
    "last_content_update" : 1479158032265,
    "path" : "~/Documents/projects/R/hw8/hw8/hw8.Rmd",
    "project_path" : "hw8.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}